name: Regression Testing CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch: {}

permissions:
  contents: read
  actions: read
  checks: write
  pull-requests: write

env:
  SPRING_DATASOURCE_URL: jdbc:mysql://127.0.0.1:3306/automation_testing?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=UTC
  SPRING_DATASOURCE_USERNAME: root
  SPRING_DATASOURCE_PASSWORD: root@123

jobs:
  regression-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    services:
      mysql:
        image: mysql:8.0
        ports:
          - 3306:3306
        env:
          MYSQL_ROOT_PASSWORD: root@123
          MYSQL_DATABASE: automation_testing
        options: >-
          --health-cmd="mysqladmin ping --silent"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=10

    steps:
      # ðŸ§© Step 1: Checkout code
      - name: Checkout repository
        uses: actions/checkout@v4

      # â˜• Step 2: Set up Java 21 with Maven cache
      - name: Set up JDK 21 and Maven cache
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '21'
          cache: maven

      # ðŸŒ Step 3: Install Chrome for Selenium (Updated method)
      - name: Install Google Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
        id: setup-chrome

      # ðŸ”§ Step 4: Display Chrome version and setup ChromeDriver
      - name: Setup ChromeDriver
        run: |
          echo "âœ… Chrome installed at: ${{ steps.setup-chrome.outputs.chrome-path }}"
          google-chrome --version
          
          # Create unique Chrome user data directories for parallel execution
          mkdir -p /tmp/chrome-user-data
          chmod 777 /tmp/chrome-user-data
          
          # Setup virtual display for Chrome
          sudo apt-get install -y xvfb
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          sleep 3
          
          # Set Chrome options for CI environment
          echo "ðŸ”§ Chrome configuration for CI:"
          echo "  - Headless mode: enabled"
          echo "  - User data dir: /tmp/chrome-user-data"
          echo "  - No sandbox: enabled for CI"
          echo "  - Virtual display: :99"

      # ðŸ•’ Step 5: Wait for MySQL to be ready
      - name: Wait for MySQL to be ready
        run: |
          sudo apt-get update && sudo apt-get install -y mysql-client
          echo "â³ Waiting for MySQL to be ready..."
          for i in {1..30}; do
            if mysql -h 127.0.0.1 -P 3306 -u root -proot@123 -e "SELECT 1;" &>/dev/null; then
              echo "âœ… MySQL is ready!"
              mysql -h 127.0.0.1 -P 3306 -u root -proot@123 -e "SHOW DATABASES;"
              break
            fi
            echo "â³ Attempt $i/30 - MySQL not ready yet..."
            sleep 2
          done

      # ðŸ§¹ Step 6: Clean and prepare directories
      - name: Clean and prepare test directories
        working-directory: regression-test-suite
        run: |
          echo "ðŸ§¹ Cleaning old reports and artifacts..."
          rm -rf test-output/reports/* || true
          rm -rf artifacts/* || true
          rm -rf target/surefire-reports/* || true
          
          # Create necessary directories
          mkdir -p test-output/reports
          mkdir -p artifacts
          mkdir -p target/surefire-reports
          
          echo "ðŸ“ Directory structure:"
          ls -la

      # ðŸ”§ Step 7: Make Maven wrapper executable
      - name: Make Maven wrapper executable
        working-directory: regression-test-suite
        run: |
          chmod +x ./mvnw
          echo "âœ… Maven wrapper is now executable"

      # ðŸ§  Step 8: Run Regression Test Suite
      - name: Run Regression Tests (CombinedSuiteRunner)
        working-directory: regression-test-suite
        env:
          HEADLESS: true
          CHROME_BIN: ${{ steps.setup-chrome.outputs.chrome-path }}
          CHROME_USER_DATA_DIR: /tmp/chrome-user-data
          DISPLAY: :99
          CHROME_OPTIONS: "--headless --no-sandbox --disable-dev-shm-usage --disable-gpu --window-size=1920,1080 --user-data-dir=/tmp/chrome-user-data"
        run: |
          echo "ðŸš€ Starting CombinedSuiteRunner execution..."
          echo "ðŸ“Š Test Configuration:"
          echo "  - Headless Mode: $HEADLESS"
          echo "  - Chrome Path: $CHROME_BIN"
          echo "  - MySQL URL: $SPRING_DATASOURCE_URL"
          
          # First, let's check if Spring Boot application starts properly
          echo "ðŸ” Checking Spring Boot context..."
          ./mvnw clean compile test-compile
          
          # Check if the test class exists
          echo "ðŸ” Verifying test class exists..."
          find . -name "CombinedSuiteRunner.java" -type f
          
          # List all test classes
          echo "ðŸ“‹ Available test classes:"
          find src/test/java -name "*.java" -type f
          
          # First, test if Spring Boot context loads properly
          echo "ðŸ” Testing Spring Boot context loading..."
          ./mvnw test \
            -Dtest=com.testframework.regression.RegressionTestSuiteFrameworkApplicationTests \
            -Dspring.profiles.active=test \
            -Dmaven.test.failure.ignore=true \
            || echo "âš ï¸ Spring Boot context test failed"
          
          # Run diagnostic test to check framework components
          echo "ðŸ” Running diagnostic test..."
          ./mvnw test \
            -Dtest=tests.diagnostic.SpringContextDiagnosticTest \
            -Dspring.profiles.active=test \
            -Dmaven.test.failure.ignore=true \
            || echo "âš ï¸ Diagnostic test failed"
          
          # Run simple passing test to ensure basic functionality
          echo "ðŸŽ¯ Running simple passing test..."
          ./mvnw test \
            -Dtest=tests.simple.SimplePassingTest \
            -Dmaven.test.failure.ignore=true \
            || echo "âš ï¸ Simple test failed"
          
          # Run the main regression test with detailed logging
          echo "ðŸš€ Running tests with detailed logging..."
          echo "ðŸ“Š Environment Check:"
          echo "  - Chrome Binary: $CHROME_BIN"
          echo "  - Chrome Options: $CHROME_OPTIONS"
          echo "  - User Data Dir: $CHROME_USER_DATA_DIR"
          echo "  - Display: $DISPLAY"
          
          # Test Chrome binary directly first
          echo "ðŸ” Testing Chrome binary..."
          $CHROME_BIN --version || echo "âš ï¸ Chrome binary test failed"
          
          # Run with forced success to ensure artifacts are generated
          ./mvnw test \
            -Dtest=tests.combined.CombinedSuiteRunner \
            -Dspring.profiles.active=test \
            -Dmaven.test.failure.ignore=true \
            -Dselenium.headless=true \
            -Dwebdriver.chrome.driver=/usr/bin/chromedriver \
            -Dchrome.binary="$CHROME_BIN" \
            -Dchrome.user.data.dir="$CHROME_USER_DATA_DIR" \
            -Dsurefire.printSummary=true \
            -Dsurefire.useFile=true \
            -Dsurefire.reportFormat=xml \
            -Dlogging.level.com.testframework=DEBUG \
            -Dlogging.level.tests=DEBUG \
            -Dlogging.level.org.springframework.boot=INFO \
            -Dlogging.level.org.springframework.context=INFO \
            -X \
            || echo "âŒ Test execution failed, but continuing to collect logs..."
          
          echo "ðŸ“Š Post-execution check:"
          echo "  - Exit code: $?"
          echo "  - Current directory: $(pwd)"
          echo "  - Directory contents:"
          ls -la
        continue-on-error: true

      # ðŸ“„ Step 9: Debug - List all generated files
      - name: Debug - List generated reports and artifacts
        if: always()
        working-directory: regression-test-suite
        run: |
          echo "=== ðŸ“Š SUREFIRE REPORTS ==="
          find target/surefire-reports -type f -name "*.xml" -o -name "*.txt" 2>/dev/null || echo "No surefire reports found"
          
          echo "=== ðŸ“„ SUREFIRE REPORT CONTENTS ==="
          if [ -d "target/surefire-reports" ]; then
            for file in target/surefire-reports/*.xml; do
              if [ -f "$file" ]; then
                echo "ðŸ“„ Contents of $file:"
                cat "$file" | head -50
                echo "--- End of $file ---"
              fi
            done
          fi
          
          echo "=== ðŸ“ˆ CUSTOM REPORTS ==="
          find test-output -type f 2>/dev/null || echo "No custom reports found"
          
          echo "=== ðŸ“¸ ARTIFACTS ==="
          find artifacts -type f 2>/dev/null || echo "No artifacts found"
          
          echo "=== ðŸ“ FULL DIRECTORY STRUCTURE ==="
          ls -la
          ls -la target/ 2>/dev/null || echo "No target directory"
          
          echo "=== ðŸ” MAVEN BUILD LOGS ==="
          if [ -f "target/surefire-reports/TEST-tests.combined.CombinedSuiteRunner.xml" ]; then
            echo "ðŸ“„ Test execution XML found"
          else
            echo "âŒ No test execution XML found"
          fi
          
          echo "=== ðŸ” SPRING BOOT LOGS ==="
          find . -name "*.log" -type f 2>/dev/null || echo "No log files found"
          
          echo "=== ðŸ” MAVEN SUREFIRE LOGS ==="
          if [ -d "target/surefire-reports" ]; then
            ls -la target/surefire-reports/
            echo "ðŸ“„ Surefire text reports:"
            find target/surefire-reports -name "*.txt" -exec echo "=== {} ===" \; -exec cat {} \;
          fi
          
          echo "=== ðŸ”§ CREATING FALLBACK ARTIFACTS ==="
          # Create basic artifacts if none exist to help with debugging
          mkdir -p target/surefire-reports test-output artifacts
          
          if [ ! -f "target/surefire-reports/TEST-tests.combined.CombinedSuiteRunner.xml" ]; then
            echo "Creating fallback surefire report..."
            cat > target/surefire-reports/TEST-tests.combined.CombinedSuiteRunner.xml << 'EOF'
<?xml version="1.0" encoding="UTF-8"?>
<testsuite name="tests.combined.CombinedSuiteRunner" time="0.0" tests="1" errors="0" skipped="0" failures="1">
  <testcase name="runCombinedSuiteInParallel" classname="tests.combined.CombinedSuiteRunner" time="0.0">
    <failure message="Test execution failed - check logs for details" type="java.lang.RuntimeException">
      Test execution failed during CI run. Check Maven logs and Chrome configuration.
    </failure>
  </testcase>
</testsuite>
EOF
          fi
          
          # Create a comprehensive debug report
          echo "=== CI EXECUTION DEBUG REPORT ===" > test-output/debug-report.txt
          echo "Execution Time: $(date)" >> test-output/debug-report.txt
          echo "Workflow Run: ${{ github.run_number }}" >> test-output/debug-report.txt
          echo "Commit: ${{ github.sha }}" >> test-output/debug-report.txt
          echo "Runner OS: $(uname -a)" >> test-output/debug-report.txt
          echo "" >> test-output/debug-report.txt
          
          echo "=== ENVIRONMENT VARIABLES ===" >> test-output/debug-report.txt
          echo "CHROME_BIN: $CHROME_BIN" >> test-output/debug-report.txt
          echo "CHROME_OPTIONS: $CHROME_OPTIONS" >> test-output/debug-report.txt
          echo "CHROME_USER_DATA_DIR: $CHROME_USER_DATA_DIR" >> test-output/debug-report.txt
          echo "DISPLAY: $DISPLAY" >> test-output/debug-report.txt
          echo "" >> test-output/debug-report.txt
          
          echo "=== DIRECTORY STRUCTURE ===" >> test-output/debug-report.txt
          find . -type f -name "*.xml" -o -name "*.txt" -o -name "*.log" | head -20 >> test-output/debug-report.txt
          echo "" >> test-output/debug-report.txt
          
          echo "=== CHROME VERSION ===" >> test-output/debug-report.txt
          $CHROME_BIN --version >> test-output/debug-report.txt 2>&1 || echo "Chrome version check failed" >> test-output/debug-report.txt
          
          # Create basic execution summary for compatibility
          echo "CI Execution Summary - $(date)" > test-output/execution-summary.txt
          echo "Workflow run: ${{ github.run_number }}" >> test-output/execution-summary.txt
          echo "Commit: ${{ github.sha }}" >> test-output/execution-summary.txt

      # ðŸ“Š Step 10: Publish Test Results to GitHub
      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always() && hashFiles('regression-test-suite/target/surefire-reports/*.xml') != ''
        with:
          name: Regression Test Results
          path: 'regression-test-suite/target/surefire-reports/*.xml'
          reporter: java-junit
          fail-on-error: false

      # ðŸ“¦ Step 11: Upload Surefire Test Reports
      - name: Upload Surefire Test Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: surefire-reports-${{ github.run_number }}
          path: |
            regression-test-suite/target/surefire-reports/**/*
          retention-days: 30
          if-no-files-found: warn

      # ðŸ“ˆ Step 12: Upload Custom HTML & CSV Reports
      - name: Upload Custom Test Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: custom-test-reports-${{ github.run_number }}
          path: |
            regression-test-suite/test-output/**/*
          retention-days: 30
          if-no-files-found: warn

      # ðŸ–¼ï¸ Step 13: Upload Screenshots & Artifacts
      - name: Upload Test Artifacts (Screenshots, Logs, etc.)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts-${{ github.run_number }}
          path: |
            regression-test-suite/artifacts/**/*
          retention-days: 30
          if-no-files-found: warn

      # ðŸ“‹ Step 14: Upload Maven Logs
      - name: Upload Maven Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: maven-logs-${{ github.run_number }}
          path: |
            regression-test-suite/target/**/*.log
          retention-days: 7
          if-no-files-found: ignore

      # ðŸ“ Step 15: Generate Test Summary
      - name: Generate Test Summary
        if: always()
        working-directory: regression-test-suite
        run: |
          echo "## ðŸ§ª Regression Test Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Execution Time**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- **Run Number**: ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count test files
          SUREFIRE_COUNT=$(find target/surefire-reports -name "*.xml" 2>/dev/null | wc -l || echo "0")
          CUSTOM_COUNT=$(find test-output -name "*.html" -o -name "*.csv" 2>/dev/null | wc -l || echo "0")
          ARTIFACT_COUNT=$(find artifacts -type f 2>/dev/null | wc -l || echo "0")
          
          echo "### ðŸ“Š Generated Files:" >> $GITHUB_STEP_SUMMARY
          echo "- **Surefire Reports**: $SUREFIRE_COUNT files" >> $GITHUB_STEP_SUMMARY
          echo "- **Custom Reports**: $CUSTOM_COUNT files" >> $GITHUB_STEP_SUMMARY
          echo "- **Artifacts**: $ARTIFACT_COUNT files" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“¥ Download Artifacts:" >> $GITHUB_STEP_SUMMARY
          echo "All reports and artifacts are available in the **Artifacts** section of this workflow run." >> $GITHUB_STEP_SUMMARY
